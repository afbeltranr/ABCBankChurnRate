{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Which ML algorithm works best to predict customer churn? :Exploratory Analysis with BigQuery & Python üöÄ\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "In this notebook, we will analyze customer churn data using Google BigQuery and Python. The dataset contains information about customers. the varibles in the dataset are:\n",
    "\n",
    "1. customer_id\n",
    "2. credit_score \n",
    "3. country\n",
    "4. gender\n",
    "5. age\n",
    "6. tenure\n",
    "7. balance\n",
    "8. products_number\n",
    "9. credit_card\n",
    "10. active_member\n",
    "11. estimated_salary\n",
    "12. churn, used as the target. 1 if the client has left the bank during some period or 0 if he/she has not.\n",
    "\n",
    "In this notebook we will set up the connection between kaggle and BigQuery, retrieve some insightful data trough queries in BigQuery, and perform some basic data analysis. We will also visualize the data using matplotlib and seaborn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the steps to migrate the data from kaggle to bigquery are shown in this diagram: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Dataset From Kaggle\n",
    "\n",
    "Now, this dataset is avaliable at kaggle [here](https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset). I selected this dataset because the dependent variable is categorical. As a chemist, I'm used to working with continuous variables for prediction problems. I find this dataset a good opportunity to apply  classification algorithms.\n",
    "\n",
    "let's then import the `kaggle API` first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the kaggle credentials are stored as secrets in this github repository, so we can extract them using `os.environ.get`. Then, we can create folder with the filepath where the json file will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "kaggle_token = os.environ.get(\"KAGGLE_JSON\")\n",
    "kaggle_config_dir = os.path.expanduser(\"~/.config/kaggle\")\n",
    "os.makedirs(kaggle_config_dir, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the file is ready to host our credentials with it's dedicated folder, we can create the write content into the file, with the filename.\n",
    "\n",
    "the `with` statement is used to open the file in write mode, hence the use of the `w` flag. \n",
    "\n",
    "finally we set the permissions of the file so only me (the owner) can read and write to it, and then we can use the kaggle API to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_json_path = os.path.join(kaggle_config_dir, \"kaggle.json\")\n",
    "with open(kaggle_json_path, \"w\") as f:\n",
    "    f.write(kaggle_token)\n",
    "\n",
    "os.chmod(kaggle_json_path, 0o600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything set up, the next step would be to authenticate and check whether or not the credentials work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no errors raised, great! Now if we provide python both the link to the dataset and the folder where it will be saved, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"gauravtopre/bank-customer-churn-dataset\"  # the categorical dependent variable dataset I described to you earlier\n",
    "download_path = \"/tmp\"  # Temporary location, not inside this repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next we can use these new variables `dataset_name` and ``:\n",
    "\n",
    "* Kaggle's API to download the dataset\n",
    "* glob to check whether the file is downloaded or not, and display it's path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "# Download dataset (ZIP file)\n",
    "api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n",
    "\n",
    "# Find the downloaded CSV file\n",
    "csv_files = glob.glob(f\"{download_path}/*.csv\")\n",
    "assert len(csv_files) > 0, \"No CSV files found. Check dataset name.\"\n",
    "csv_file_path = csv_files[0]  \n",
    "\n",
    "print(f\"‚úÖ Dataset downloaded: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done! now we can move on to the next part, which is uploading the dataset to bigquery to excecute queries and explore the dataset. for this, we will use a gcp service account which I already set up, and saved it's key as a secret in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Service Account Credentials and Initializing BigQuery Client\n",
    "\n",
    "Since both the credentials for kaggle and the service account are stored as secrets in this repository, we can extract them using `os.environ.get`. \n",
    "Then, we can create folder with the filepath where the json file will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Save GCP service account JSON to a temporary file and set the path\n",
    "gcp_key = os.environ.get(\"GCP_SA_KEY\")\n",
    "gcp_path = \"/tmp/sa_credentials.json\"\n",
    " \n",
    "with open(gcp_path, \"w\") as f:\n",
    "    f.write(gcp_key)\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = gcp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "The folling code initializes the BigQuery client using the service account credentials stored in the environment variable `GOOGLE_APPLICATION_CREDENTIALS`. This allows us to interact with BigQuery and run queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.bigquery as bigquery\n",
    "client = bigquery.Client()\n",
    "print(client.project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `client` object will allow us to perform operations like:\n",
    "\n",
    "- Running SQL queries on BigQuery Datasets (My main goal on pursuing this specific path)\n",
    "- Creating datasets and tables (this will be needed to fetch)\n",
    "- fetch query results as `pandas` DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = client.project # This is already defined by the service account\n",
    "dataset_id = f\"{project_id}.churn_analysis\"\n",
    "table_id = \"Kaggle_churn\"\n",
    "full_table_id = f\"{dataset_id}.{table_id}\"\n",
    "print(full_table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined that unique identifier for the data table, and the dataset whithin it will be saved, we can go ahead and create the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_dataset(dataset_id, exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now check if there is any tables within the dataset. I think there should not because we have not used the kaggle API yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {table.table_id for table in client.list_tables(dataset_id)}\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok this set comprehension (which I find tremendously efficient) loops trough all the tables within the dataset named `dataset_id`. indeed since right now we haven't used the kaggle functions there is no table yet. Let's get to that.\n",
    "\n",
    "ok the next chunk of code will upload the dataset to bigquery. \n",
    "\n",
    "this process need several configurations to be set, so we are going to save these in the `job_config` variable as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig(\n",
    "    autodetect = True,\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that `job_config` variable will tell the `load_table_from_file` function which schema to use, and what configuration the table load should have.\n",
    "\n",
    "I've been trough this process many times inside the bigquery site, and every time I felt like selecting options from drop down menus, and pushing buttons was not as reproducible as I would like it to be. Now I'm glad it can be written down in code and anyone can use this for their needs (assuming someone besides me reads this ü§£). \n",
    "\n",
    "Now let's do the upload:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(csv_file_path, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, full_table_id, job_config=job_config)\n",
    "\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Queries in BigQuery\n",
    "\n",
    "OK! finally, we have the dataset in biquery and we can query it so it gives us insight on how we should address the data, and which model we could use to predict the bank churn. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## EDA with SQL Queries\n",
    "\n",
    "### Data Validation\n",
    "\n",
    "First we can check if there is any missing values. This can be done both with SQL and python, let's do both for sake of comparison.\n",
    "\n",
    "#### Missing values with SQL\n",
    "\n",
    "Since using SQL in bigquery there is no way to dinamically loop trough a dataset column, each column must be checked. if we were only using SQL it would look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "`SELECT \n",
    "    SUM(CASE WHEN credit_score IS NULL THEN 1 ELSE 0 END) AS missing_credit_score,\n",
    "    ... -- other columns as needed`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the CASE statement begins a control flow structure that checks whether or not there is null values in the column, and then the sum fuction adds them all up. Having checked that, we can define the query to pass it to the BigQuery client and execute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_query = \"\"\"\n",
    "SELECT\n",
    "  SUM(CASE WHEN credit_score IS NULL THEN 1 ELSE 0 END) AS missing_credit_score,\n",
    "  SUM(CASE WHEN country IS NULL THEN 1 ELSE 0 END) AS missing_country,\n",
    "  SUM(CASE WHEN gender IS NULL THEN 1 ELSE 0 END) AS missing_gender,\n",
    "  SUM(CASE WHEN age IS NULL THEN 1 ELSE 0 END) AS missing_age,\n",
    "  SUM(CASE WHEN tenure IS NULL THEN 1 ELSE 0 END) AS missing_tenure,\n",
    "  SUM(CASE WHEN balance IS NULL THEN 1 ELSE 0 END) AS missing_balance,\n",
    "  SUM(CASE WHEN products_number IS NULL THEN 1 ELSE 0 END) AS missing_products_number,\n",
    "  SUM(CASE WHEN credit_card IS NULL THEN 1 ELSE 0 END) AS missing_credit_card,\n",
    "  SUM(CASE WHEN active_member IS NULL THEN 1 ELSE 0 END) AS missing_active_member,\n",
    "  SUM(CASE WHEN estimated_salary IS NULL THEN 1 ELSE 0 END) AS missing_estimated_salary,\n",
    "  SUM(CASE WHEN churn IS NULL THEN 1 ELSE 0 END) AS missing_churn\n",
    "FROM\n",
    "  `kagglebigquerybankchurn.churn_analysis.Kaggle_churn`\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data cleaning SQL queries with python:\n",
    "\n",
    "above we just repeated the same structure that is neede to retrieve the amount of missing values with SQL in a column, for all columns within the dataset. Conversely, we can use python to automate query generation, as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First we use the bigquery `client` we generated to retrieve the table's metadata, where column names are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ref = client.get_table(full_table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* then, we can use a list comprehension, which is a compact loop that generates a list of strings, to save the column names in the columns variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [schema_field.name for schema_field in table_ref.schema if schema_field.name != \"customer_id\"]\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the actual names of the columns, the same query can be generated using a list comprehension as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_expression = [f\"SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) AS missing_{col}\" for col in columns]\n",
    "print(select_expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have each of the lines for the query, we can join them together as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_query_python =  f\"\"\"\n",
    "SELECT\n",
    "    {', '.join(select_expression)}\n",
    "FROM\n",
    "    `{full_table_id}`\n",
    "\"\"\"\n",
    "print(missing_values_query_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run both queries, `missing_values_query` and `missing_values_query_python`, to see if they yield the same results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First the query we wrote manually: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardcoded_job = client.query(missing_values_query)\n",
    "hardcoded_result = hardcoded_job.to_dataframe()\n",
    "display(hardcoded_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* and now the query generated with python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_job = client.query(missing_values_query_python)\n",
    "python_result = python_job.to_dataframe()\n",
    "display(python_result) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! we can visually confirm that tere is no missing values in this dataset. if this process were to be automated, this visual inspection would not be enough. thankfully python allows us to compare the results of both queries, and check if they are equal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do both queries give the same result:? True\n"
     ]
    }
   ],
   "source": [
    "comparison = hardcoded_result.equals(python_result)\n",
    "print(f\"do both queries give the same result:? {comparison}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that's great, missing values is one of the first data integrity checks that should be done and now we have done it.\n",
    "\n",
    "### Checking for duplicates in SQL\n",
    "\n",
    "SQL makes it easy to check for duplicates as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 10000\n"
     ]
    }
   ],
   "source": [
    "duplicates_query = \"\"\"\n",
    "\n",
    "SELECT customer_id, COUNT(*) AS count\n",
    "FROM `kagglebigquerybankchurn.churn_analysis.Kaggle_churn`\n",
    "GROUP BY customer_id\n",
    "HAVING count > 1; \n",
    "\"\"\"\n",
    "\n",
    "duplicates_job = client.query(duplicates_query)\n",
    "duplicates_result = duplicates_job.to_dataframe()\n",
    "print(f\"Number of duplicate rows: {len(duplicates_result)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Complete Duplicate Rows (All Fields)\n",
    "\n",
    "The previous query only checked for duplicate customer IDs. But we need to verify if rows with duplicate customer IDs also have identical content across ALL fields. Let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique row combinations that appear more than once: 10000\n",
      "Total duplicate rows (sum of all duplicates): 110000\n",
      "\n",
      "First 5 complete duplicate row combinations:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "customer_id",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "credit_score",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "gender",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "age",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "tenure",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "balance",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "products_number",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "credit_card",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "active_member",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "estimated_salary",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "churn",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "duplicate_count",
         "rawType": "Int64",
         "type": "integer"
        }
       ],
       "ref": "8ae0cb13-78d4-4b9e-a0d1-26337f6234bf",
       "rows": [
        [
         "0",
         "15647091",
         "725",
         "Germany",
         "Male",
         "19",
         "0",
         "75888.2",
         "1",
         "0",
         "0",
         "45613.75",
         "0",
         "11"
        ],
        [
         "1",
         "15713826",
         "613",
         "Germany",
         "Female",
         "20",
         "0",
         "117356.19",
         "1",
         "0",
         "0",
         "113557.7",
         "1",
         "11"
        ],
        [
         "2",
         "15633840",
         "781",
         "France",
         "Male",
         "20",
         "0",
         "125023.1",
         "2",
         "1",
         "1",
         "108301.45",
         "0",
         "11"
        ],
        [
         "3",
         "15769915",
         "643",
         "Spain",
         "Female",
         "20",
         "0",
         "133313.34",
         "1",
         "1",
         "1",
         "3965.69",
         "0",
         "11"
        ],
        [
         "4",
         "15652674",
         "539",
         "France",
         "Male",
         "20",
         "0",
         "83459.86",
         "1",
         "1",
         "1",
         "146752.67",
         "0",
         "11"
        ]
       ],
       "shape": {
        "columns": 13,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>products_number</th>\n",
       "      <th>credit_card</th>\n",
       "      <th>active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>churn</th>\n",
       "      <th>duplicate_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15647091</td>\n",
       "      <td>725</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>75888.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45613.75</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15713826</td>\n",
       "      <td>613</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>117356.19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113557.70</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15633840</td>\n",
       "      <td>781</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>125023.10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>108301.45</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15769915</td>\n",
       "      <td>643</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>133313.34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3965.69</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15652674</td>\n",
       "      <td>539</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>83459.86</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>146752.67</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  credit_score  country  gender  age  tenure    balance  \\\n",
       "0     15647091           725  Germany    Male   19       0   75888.20   \n",
       "1     15713826           613  Germany  Female   20       0  117356.19   \n",
       "2     15633840           781   France    Male   20       0  125023.10   \n",
       "3     15769915           643    Spain  Female   20       0  133313.34   \n",
       "4     15652674           539   France    Male   20       0   83459.86   \n",
       "\n",
       "   products_number  credit_card  active_member  estimated_salary  churn  \\\n",
       "0                1            0              0          45613.75      0   \n",
       "1                1            0              0         113557.70      1   \n",
       "2                2            1              1         108301.45      0   \n",
       "3                1            1              1           3965.69      0   \n",
       "4                1            1              1         146752.67      0   \n",
       "\n",
       "   duplicate_count  \n",
       "0               11  \n",
       "1               11  \n",
       "2               11  \n",
       "3               11  \n",
       "4               11  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SQL query to check for complete duplicate rows (all fields identical)\n",
    "complete_duplicates_query = \"\"\"\n",
    "SELECT \n",
    "    customer_id, credit_score, country, gender, age, tenure, balance, \n",
    "    products_number, credit_card, active_member, estimated_salary, churn,\n",
    "    COUNT(*) as duplicate_count\n",
    "FROM `kagglebigquerybankchurn.churn_analysis.Kaggle_churn`\n",
    "GROUP BY \n",
    "    customer_id, credit_score, country, gender, age, tenure, balance, \n",
    "    products_number, credit_card, active_member, estimated_salary, churn\n",
    "HAVING COUNT(*) > 1\n",
    "ORDER BY duplicate_count DESC\n",
    "\"\"\"\n",
    "\n",
    "complete_duplicates_job = client.query(complete_duplicates_query)\n",
    "complete_duplicates_result = complete_duplicates_job.to_dataframe()\n",
    "\n",
    "print(f\"Number of unique row combinations that appear more than once: {len(complete_duplicates_result)}\")\n",
    "print(f\"Total duplicate rows (sum of all duplicates): {complete_duplicates_result['duplicate_count'].sum()}\")\n",
    "\n",
    "# Show first few examples\n",
    "if len(complete_duplicates_result) > 0:\n",
    "    print(\"\\nFirst 5 complete duplicate row combinations:\")\n",
    "    display(complete_duplicates_result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above code identified that there is 10000 customer ids that appear more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARISON ===\n",
      "Customer IDs with duplicates: 10000\n",
      "Complete duplicate row combinations: 10000\n",
      "‚úÖ All duplicate customer IDs have identical content across all fields!\n",
      "‚úÖ Safe to remove duplicates - they are true duplicates\n"
     ]
    }
   ],
   "source": [
    "# Let's compare the results\n",
    "print(\"=== COMPARISON ===\")\n",
    "print(f\"Customer IDs with duplicates: {len(duplicates_result)}\")\n",
    "print(f\"Complete duplicate row combinations: {len(complete_duplicates_result)}\")\n",
    "\n",
    "# Check if they match\n",
    "if len(duplicates_result) == len(complete_duplicates_result):\n",
    "    print(\"‚úÖ All duplicate customer IDs have identical content across all fields!\")\n",
    "    print(\"‚úÖ Safe to remove duplicates - they are true duplicates\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some customer IDs have different content - need to investigate further\")\n",
    "    print(\"‚ö†Ô∏è  Cannot safely remove duplicates without further analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can retrieve the complete dataset using the following query:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_query   = \"\"\"\n",
    "SELECT *\n",
    "FROM `kagglebigquerybankchurn.churn_analysis.Kaggle_churn`\n",
    "\"\"\"\n",
    "df_job = client.query(df_query)\n",
    "df = df_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Duplicate Analysis\n",
    "\n",
    "Now let's perform the same duplicate analysis using Python with pandas to compare the power and ease of both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PYTHON DUPLICATE ANALYSIS ===\n",
      "Customer IDs with duplicates: 10000\n",
      "Total rows that are complete duplicates: 110000\n",
      "Unique combinations of complete duplicates: 100000\n",
      "\n",
      "=== COMPARISON WITH SQL RESULTS ===\n",
      "SQL - Customer IDs with duplicates: 10000\n",
      "Python - Customer IDs with duplicates: 10000\n",
      "Match: ‚úÖ\n",
      "\n",
      "Complete duplicate rows grouped by customer_id:\n",
      "Number of customer IDs in complete duplicates: 10000\n",
      "‚úÖ All duplicate customer IDs have identical content across all fields!\n",
      "‚úÖ Safe to remove duplicates using pandas.drop_duplicates()\n"
     ]
    }
   ],
   "source": [
    "# Python duplicate analysis using the full dataset (df)\n",
    "print(\"=== PYTHON DUPLICATE ANALYSIS ===\")\n",
    "\n",
    "# 1. Check for duplicate customer IDs\n",
    "customer_id_duplicates = df.groupby('customer_id').size()\n",
    "customer_ids_with_duplicates = customer_id_duplicates[customer_id_duplicates > 1]\n",
    "\n",
    "print(f\"Customer IDs with duplicates: {len(customer_ids_with_duplicates)}\")\n",
    "\n",
    "# 2. Check for complete duplicate rows (all fields identical)\n",
    "complete_duplicates = df.duplicated(keep=False)  # keep=False marks all duplicates as True\n",
    "duplicate_rows = df[complete_duplicates]\n",
    "\n",
    "print(f\"Total rows that are complete duplicates: {complete_duplicates.sum()}\")\n",
    "print(f\"Unique combinations of complete duplicates: {len(df[df.duplicated(keep='first')])}\")\n",
    "\n",
    "# 3. Compare customer ID duplicates vs complete row duplicates\n",
    "print(\"\\n=== COMPARISON WITH SQL RESULTS ===\")\n",
    "print(f\"SQL - Customer IDs with duplicates: {len(duplicates_result)}\")\n",
    "print(f\"Python - Customer IDs with duplicates: {len(customer_ids_with_duplicates)}\")\n",
    "print(f\"Match: {'‚úÖ' if len(duplicates_result) == len(customer_ids_with_duplicates) else '‚ùå'}\")\n",
    "\n",
    "# 4. Check if all customer ID duplicates are also complete row duplicates\n",
    "if complete_duplicates.sum() > 0:\n",
    "    # Group complete duplicates by customer_id to see the pattern\n",
    "    duplicate_customer_analysis = duplicate_rows.groupby('customer_id').size()\n",
    "    print(f\"\\nComplete duplicate rows grouped by customer_id:\")\n",
    "    print(f\"Number of customer IDs in complete duplicates: {len(duplicate_customer_analysis)}\")\n",
    "    \n",
    "    # Verify if duplicate customer IDs have identical content\n",
    "    if len(customer_ids_with_duplicates) == len(duplicate_customer_analysis):\n",
    "        print(\"‚úÖ All duplicate customer IDs have identical content across all fields!\")\n",
    "        print(\"‚úÖ Safe to remove duplicates using pandas.drop_duplicates()\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Some customer IDs have different content - investigate further!\")\n",
    "else:\n",
    "    print(\"No complete duplicate rows found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates Safely\n",
    "\n",
    "If our analysis confirms that duplicate customer IDs have identical content across all fields, we can safely remove duplicates. Here's how to do it in both SQL and Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 110000\n",
      "Deduplicated dataset size (SQL): 10000\n",
      "Rows removed: 100000\n"
     ]
    }
   ],
   "source": [
    "# Method 1: SQL - Create a deduplicated dataset using ROW_NUMBER()\n",
    "dedup_sql_query = \"\"\"\n",
    "SELECT \n",
    "    customer_id, credit_score, country, gender, age, tenure, balance, \n",
    "    products_number, credit_card, active_member, estimated_salary, churn\n",
    "FROM (\n",
    "    SELECT *,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY customer_id\n",
    "            ORDER BY customer_id\n",
    "        ) as row_num\n",
    "    FROM `kagglebigquerybankchurn.churn_analysis.Kaggle_churn`\n",
    ") \n",
    "WHERE row_num = 1\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query to get deduplicated data\n",
    "dedup_sql_job = client.query(dedup_sql_query)\n",
    "dedup_sql_df = dedup_sql_job.to_dataframe()\n",
    "\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"Deduplicated dataset size (SQL): {len(dedup_sql_df)}\")\n",
    "print(f\"Rows removed: {len(df) - len(dedup_sql_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Python deduplication:\n",
      "Original dataset size: 110000\n",
      "Deduplicated dataset size (Python): 10000\n",
      "Rows removed: 100000\n",
      "\n",
      "Both methods give same result: ‚úÖ\n",
      "\n",
      "Final verification:\n",
      "Unique customer IDs in SQL deduplicated data: 10000\n",
      "Unique customer IDs in Python deduplicated data: 10000\n",
      "Total rows in deduplicated data: 10000\n",
      "‚úÖ Perfect! Each customer ID now appears exactly once.\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Python - Remove duplicates using pandas\n",
    "dedup_python_df = df.drop_duplicates()\n",
    "\n",
    "print(f\"\\nPython deduplication:\")\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"Deduplicated dataset size (Python): {len(dedup_python_df)}\")\n",
    "print(f\"Rows removed: {len(df) - len(dedup_python_df)}\")\n",
    "\n",
    "# Verify both methods give the same result\n",
    "print(f\"\\nBoth methods give same result: {'‚úÖ' if len(dedup_sql_df) == len(dedup_python_df) else '‚ùå'}\")\n",
    "\n",
    "# Final verification - check unique customer IDs in deduplicated data\n",
    "print(f\"\\nFinal verification:\")\n",
    "print(f\"Unique customer IDs in SQL deduplicated data: {dedup_sql_df['customer_id'].nunique()}\")\n",
    "print(f\"Unique customer IDs in Python deduplicated data: {dedup_python_df['customer_id'].nunique()}\")\n",
    "print(f\"Total rows in deduplicated data: {len(dedup_python_df)}\")\n",
    "\n",
    "# If they match, we have successfully removed duplicates\n",
    "if dedup_python_df['customer_id'].nunique() == len(dedup_python_df):\n",
    "    print(\"‚úÖ Perfect! Each customer ID now appears exactly once.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Still some issues with the deduplication process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: SQL vs Python for Duplicate Handling\n",
    "\n",
    "#### **SQL Approach:**\n",
    "**Advantages:**\n",
    "- ‚úÖ Handles large datasets efficiently (server-side processing)\n",
    "- ‚úÖ Explicit control over duplicate detection logic\n",
    "- ‚úÖ Can be integrated into ETL pipelines\n",
    "- ‚úÖ Memory efficient for very large datasets\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå More verbose syntax (need to specify all columns)\n",
    "- ‚ùå Less flexible for complex duplicate detection rules\n",
    "- ‚ùå Requires good understanding of window functions\n",
    "\n",
    "#### **Python/Pandas Approach:**\n",
    "**Advantages:**\n",
    "- ‚úÖ Simple, concise syntax (`df.drop_duplicates()`)\n",
    "- ‚úÖ Flexible - easy to specify subset of columns or custom logic\n",
    "- ‚úÖ Integrated with data analysis workflow\n",
    "- ‚úÖ Rich ecosystem for further data validation\n",
    "\n",
    "**Disadvantages:**\n",
    "- ‚ùå Memory intensive for very large datasets\n",
    "- ‚ùå Slower for massive datasets compared to SQL\n",
    "- ‚ùå Requires loading full dataset into memory\n",
    "\n",
    "#### **Best Practice Recommendation:**\n",
    "- Use **SQL** for initial data cleaning in production ETL pipelines\n",
    "- Use **Python** for exploratory analysis and validation\n",
    "- Always validate that both approaches give consistent results (as we did above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is a hundred thousand total rows, and just ten thousand unique customer ids, that could mean that each customer id has 10 entries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis in python\n",
    "\n",
    "now let's see how the variables are related to the churn rate. For that, we will create the main dataframe again, but will not limit the amount of rows to ten, this dataframe will be named `full_df`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_query = \"\"\"\n",
    "SELECT *\n",
    "FROM `kagglebigquerybankchurn.churn_analysis.Kaggle_churn`\n",
    "\"\"\"\n",
    "full_query_job = client.query(full_query)\n",
    "full_df = full_query_job.to_dataframe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's really quick check if there are any missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check information about the variables with the `.info` and `.describe` functions as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to this summary table, we can identify that there is two groups of variables:\n",
    "\n",
    "### Continuous variables:\n",
    "- Credit_score\n",
    "- Age\n",
    "- Estimated_salary\n",
    "- Balance\n",
    "\n",
    "Let's examine these variables further by visualizing their distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the canvas for four plots (2 rows, 4 columns)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up a larger figure with a grid of 4x2 subplots\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(4, 2, height_ratios=[3, 1, 3, 1])\n",
    "\n",
    "# Set the figure title\n",
    "fig.suptitle('Continuous Variables Distributions', fontsize=20, y=0.95)\n",
    "\n",
    "# Define variables and titles for the plots\n",
    "variables = ['credit_score', 'age', 'estimated_salary', 'balance']\n",
    "titles = ['Credit Score Distribution', 'Age Distribution', 'Estimated Salary Distribution', 'Balance Distribution']\n",
    "\n",
    "# Loop through variables and plot\n",
    "for i, (var, title) in enumerate(zip(variables, titles)):\n",
    "    # Histogram\n",
    "    ax_hist = fig.add_subplot(gs[i // 2 * 2, i % 2])\n",
    "    sns.histplot(data=full_df, x=var, kde=True, ax=ax_hist)\n",
    "    ax_hist.set_title(title)\n",
    "    \n",
    "    # Boxplot\n",
    "    ax_box = fig.add_subplot(gs[i // 2 * 2 + 1, i % 2])\n",
    "    sns.boxplot(data=full_df, x=var, color='orange', ax=ax_box)\n",
    "    ax_box.set_title('')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to these visualizations we can comment some things about these four continuous variables, to consider in the modelling stage:\n",
    "\n",
    "1. **Credit Score**\n",
    "    - Is normally distributed, good for modelling, and also complies with the assumptions of most statistical tests.\n",
    "    - Supports the assumptions of most hypothesis tests (that's great, just What I Learned in the regression course of the *Google Advanced Data Analytics Professional Certificate*).\n",
    "\n",
    "    **References**:\n",
    "    - Hosmer Jr, D. W., et al. (2013). [\"Applied Logistic Regression\"](https://books.google.com.co/books?hl=en&lr=&id=bRoxQBIZRd4C&oi=fnd&pg=PR13&dq=Hosmer+Jr,+D.+W.,+et+al.+(2013).+%22Applied+Logistic+Regression%22+-+Discusses+normality+assumptions&ots=kM4Otl6Sb5&sig=9Q0GI1KojiWmqTd861azWkxeWm8&redir_esc=y#v=onepage&q&f=false) - Discusses normality assumptions.\n",
    "    - Altman, N., & Krzywinski, M. (2016). [\"Points of Significance: Analyzing outliers\"](https://go.gale.com/ps/i.do?id=GALE%7CA461963379&sid=googleScholar&v=2.1&it=r&linkaccess=abs&issn=15487091&p=AONE&sw=w&userGroupName=anon%7E5d49140c&aty=open-web-entry) Nature Methods.\n",
    "\n",
    "2. **Age (normal with a right tail)**\n",
    "    - Age distributions are commonly found to have a right-skewed distribution (this makes sense to me because older people tend to die and then the amount of older people decreases).\n",
    "    - May require log transformation for a linear model.\n",
    "\n",
    "    **References**:\n",
    "    - Cox, D. R., & Snell, E. J. (1989). [\"Analysis of Binary Data\"](https://www.taylorfrancis.com/books/mono/10.1201/9781315137391/analysis-binary-data-cox) (2nd ed.). Chapman and Hall/CRC.\n",
    "    - Faraway, J. J. (2016). [\"Extending the Linear Model with R\"](https://www.google.com.co/books/edition/Extending_the_Linear_Model_with_R/XAzYCwAAQBAJ?hl=en&gbpv=1&dq=Faraway,+J.+J.+(2016).+%22Extending+the+Linear+Model+with+R%22+(2nd+ed.),+Chapman+and+Hall/CRC&pg=PR2&printsec=frontcover) (2nd ed.), Chapman and Hall/CRC.\n",
    "\n",
    "3. **Estimated Salary (Uniform)**\n",
    "    - Uniform distribution. This is weird for me, and checking in literature uniform distributions indeed are due to:\n",
    "        - Data preprocessing/binning could work to enhance its usability.\n",
    "        - Potential sampling bias.\n",
    "\n",
    "    **References**:\n",
    "    - Cohen, J., et al. (2003). \"Applied Multiple Regression/Correlation Analysis.\"\n",
    "    - Friedman, J., et al. (2001). \"The Elements of Statistical Learning\" - Discussion of feature distributions.\n",
    "\n",
    "4. **Balance (Normal, with many values around zero)**\n",
    "    - Well, it seems like in financial data this is normal, and I understand, many people will have accounts with zero balance.\n",
    "    - Statistical implications:\n",
    "        - Need for specialized modeling approaches.\n",
    "        - Consider two-part models (zero vs non-zero).\n",
    "        - May require transformation for non-zero values.\n",
    "\n",
    "    **References**:\n",
    "\n",
    "    - Min, Y., & Agresti, A. (2002). \"Modeling nonnegative data with clumping at zero: A survey.\" Journal of the Iranian Statistical Society, 1(1-2), 7-33.\n",
    "\n",
    "\n",
    "    - Fletcher, D., & Dixon, P. M. (2012). \"Modelling data with excess zeros and measurement error: Application to evaluating relationships between abundances of multiple species.\" Biometrics, 68(1), 270-278.\n",
    "\n",
    "\n",
    "    - Tu, W. (2006). \"Zero-Inflated Data.\" Encyclopedia of Environmetrics, 6.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a summary, what we can do with these variables is: \n",
    "- **Credit Score:** Use as-is in logistic regression\n",
    "- **Age:** Consider log transformation\n",
    "- **Salary:** Consider binning or categorical transformation\n",
    "- **Balance:** Create binary flag for zero values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to categorical variables, I want to see how the dependent variable, churn, is related to these four variables in a similar fashion, son now we will again draw four plots in the same canvas, but this time the will be scatter plots instead of histograms, keeping the boxplot of the independent variable at the bottom:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the canvas for four plots (2 rows, 4 columns)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up a larger figure with a grid of 4x2 subplots\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(4, 2, height_ratios=[3, 1, 3, 1])\n",
    "\n",
    "# Set the figure title\n",
    "fig.suptitle('Churn Rate vs Continuous Variables', fontsize=20, y=0.95)\n",
    "\n",
    "# Define variables and titles for the plots\n",
    "variables = ['credit_score', 'age', 'estimated_salary', 'balance']\n",
    "titles = ['Churn Rate vs Credit Score', 'Churn Rate vs Age', 'Churn Rate vs Estimated Salary', 'Churn Rate vs Balance']\n",
    "\n",
    "# Loop through variables and plot\n",
    "for i, (var, title) in enumerate(zip(variables, titles)):\n",
    "    # Scatterplot\n",
    "    ax_scatter = fig.add_subplot(gs[i // 2 * 2, i % 2])\n",
    "    sns.scatterplot(data=full_df, x=var, y='churn', alpha=0.5, ax=ax_scatter)\n",
    "    ax_scatter.set_title(title)\n",
    "    ax_scatter.set_ylabel('Churn Rate')\n",
    "    \n",
    "    # Boxplot\n",
    "    ax_box = fig.add_subplot(gs[i // 2 * 2 + 1, i % 2])\n",
    "    sns.boxplot(data=full_df, x=var, color='orange', ax=ax_box)\n",
    "    ax_box.set_title('')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a larger figure with a grid of 3x2 subplots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 15))\n",
    "\n",
    "# Set the figure title\n",
    "fig.suptitle('Churn Rate vs Categorical Variables', fontsize=20, y=0.95)\n",
    "\n",
    "# Define categorical variables and titles for the plots\n",
    "cat_variables = ['gender', 'country', 'credit_card', 'active_member', 'products_number', 'tenure']\n",
    "titles = ['Churn vs Gender', 'Churn vs Country', 'Churn vs Credit Card', \n",
    "          'Churn vs Active Member', 'Churn vs Products Number', 'Churn vs Tenure']\n",
    "\n",
    "# Loop through variables and plot\n",
    "for ax, var, title in zip(axes.flatten(), cat_variables, titles):\n",
    "    sns.barplot(data=full_df, x=var, y='churn', ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Churn Rate')\n",
    "    ax.set_xlabel(var.capitalize())\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Categorical Variables and Their Relationship with Churn\n",
    "\n",
    "### Key Observations and Modeling Implications:\n",
    "\n",
    "1. **Gender**\n",
    "   - Slight imbalance in distribution (more females than males)\n",
    "   - Similar churn rates between groups\n",
    "   - **Modeling Decision**: Include as-is, but consider:\n",
    "     - Using stratification in train-test split\n",
    "     - Checking for interaction effects with other variables\n",
    "\n",
    "2. **Country**\n",
    "   - Uneven distribution (Germany ~2x France/Spain)\n",
    "   - Different churn patterns across countries\n",
    "   - **Modeling Decision**: \n",
    "     - Convert to dummy variables\n",
    "     - Consider weighted sampling if using tree-based models\n",
    "     - Important feature to keep due to visible variation in churn rates\n",
    "\n",
    "3. **Credit Card Status**\n",
    "   - Balanced distribution\n",
    "   - Similar churn rates between holders/non-holders\n",
    "   - **Modeling Decision**: Consider excluding due to low predictive power\n",
    "     - Run feature importance check to confirm\n",
    "     - Could be kept for interaction effects\n",
    "\n",
    "4. **Active Member Status**\n",
    "   - Strong predictor: 2x churn rate for inactive members\n",
    "   - Clear separation between groups\n",
    "   - **Modeling Decision**: \n",
    "     - Definitely keep\n",
    "     - Consider as primary feature\n",
    "     - Check for interactions with balance and products_number\n",
    "\n",
    "5. **Products Number**\n",
    "   - Higher churn for 3-4 products\n",
    "   - Non-linear relationship with churn\n",
    "   - **Modeling Decision**:\n",
    "     - Keep as categorical rather than continuous\n",
    "     - Consider binning into: low (1-2) and high (3-4) products\n",
    "     - Test both original and binned versions\n",
    "\n",
    "6. **Tenure**\n",
    "   - Uniform distribution across groups\n",
    "   - Weak relationship with churn\n",
    "   - **Modeling Decision**:\n",
    "     - Test both as continuous and categorical\n",
    "     - Consider binning into meaningful groups (new/established customers)\n",
    "     - Check for interaction with active_member\n",
    "     \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
