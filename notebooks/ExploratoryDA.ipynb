{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Customer Churn Analysis with BigQuery & Python ðŸš€\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "In this notebook, we will analyze customer churn data using Google BigQuery and Python. The dataset contains information about customers. the varibles in the dataset are:\n",
    "\n",
    "1. customer_id\n",
    "2. credit_score \n",
    "3. country\n",
    "4. gender\n",
    "5. age\n",
    "6. tenure\n",
    "7. balance\n",
    "8. products_number\n",
    "9. credit_card\n",
    "10. active_member\n",
    "11. estimated_salary\n",
    "12. churn, used as the target. 1 if the client has left the bank during some period or 0 if he/she has not.\n",
    "\n",
    "In this notebook we will set up the connection between kaggle and BigQuery, load the data from BigQuery, and perform some basic data analysis. We will also visualize the data using matplotlib and seaborn.\n",
    "\n",
    "## **Import Libraries**\n",
    "\n",
    "For this first part we will use `os` to set the environment variables, `pandas` to manipulate the data and `google.cloud` to connect to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Service Account Credentials\n",
    "\n",
    "For this project, I wanted to learn how to manage kaggle datasets using bigQuery.Turns out GCP provides service acounts, which can be used to access BigQuery datasets.\n",
    "\n",
    "First we have to set the environment variables for the service account credentials.\n",
    "\n",
    "I decided to do this by downloading the key directly from GCP, and saving it on the current codespaces virtual machine I'm using for development. For this process to be secure, I added the folder where this key is stored to the .gitignore file, so it won't be uploaded to GitHub. \n",
    "\n",
    "Then, let's set an environment variable where the path to the .json file is saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/workspaces/ABCBankChurnRate/.config/sa_credentials.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing BigQuery Client\n",
    "\n",
    "Now, a variable called `client` which is an instance of the `google.cloud.bigquery.Client` class, has it's own methods and attributes. One of it's attributes is `.project`. Printing it will confirm that the client is authenticated, and connected to the GCP project I set up for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kagglebigquerybankchurn\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client()\n",
    "print(client.project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `client` object will allow us to perform operations like:\n",
    "\n",
    "- Running SQL queries on BigQuery Datasets (My main goal on pursuing this specific path)\n",
    "- Creating datasets and tables (this will be needed to fetch)\n",
    "- fetch query results as `pandas` DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Dataset From Kaggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this dataset is avaliable at kaggle [here](https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset). I selected this dataset because the dependent variable is categorical. From my background in analytical chemistry I'm used to work with continuous dependent variables. Then, this is a great oportunity for me to learn how these different systems behave.\n",
    "\n",
    "let's then import the `kaggle API` first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`glob` will be used to handle the datasets temporarily. I think datasets should not be saved in repositories for efficiency, so we well see below how to address this.\n",
    "\n",
    "Now, we will temporarily download the dataset using my "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading dataset to bigquery\n",
    "\n",
    "In the Google Data Analytics Professional certificate, I had the chance to work with bigquery and I found it amazing. From those times I remember each table is within a dataset, and it should have it's own unique identifier. We will need that now, so let's set it up.\n",
    "\n",
    "In the following code we will define the `dataset_id` and `table_id` our kaggle dataset will have inside bigquery, and we will use to run our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = client.project # This is already defined by the service account\n",
    "dataset_id = f\"{project_id}.churn_analysis\"\n",
    "table_id = \"Kaggle_churn\"\n",
    "full_table_id = f\"{dataset_id}.{table_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined that unique identifier for the data table, and the dataset whithin it will be saved, we can go ahead and create the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(DatasetReference('kagglebigquerybankchurn', 'churn_analysis'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_dataset(dataset_id, exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've been trough this process many times inside the bigquery site, and every time I felt like selecting options from drop down menus, and pushing buttons was not as reproducible as I would like it to be. Now I'm glad it can be written down in code and anyone can use this for their needs (assuming someone besides me reads this ðŸ¤£). \n",
    "\n",
    "\n",
    "Now check if there is any tables within the dataset. I think there should not because we have not used the kaggle API yet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "tables = {table.table_id for table in client.list_tables(dataset_id)}\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok this set comprehension (which I find tremendously efficient) loops trough all the tables within the dataset named `dataset_id`. indeed since right now we haven't used the kaggle functions there is no table yet. Let's get to that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
