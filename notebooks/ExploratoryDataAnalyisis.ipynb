{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/workspaces/ABCBankChurnRate/.config/sa_credentials.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kagglebigquerybankchurn\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client()\n",
    "print(client.project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Kaggle credentials are set in the notebook.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Manually load environment variables inside the notebook (if not set)\n",
    "if not os.getenv(\"KAGGLE_USERNAME\") or not os.getenv(\"KAGGLE_KEY\"):\n",
    "    os.environ[\"KAGGLE_USERNAME\"] = \"your_kaggle_username\"\n",
    "    os.environ[\"KAGGLE_KEY\"] = \"your_kaggle_api_key\"\n",
    "\n",
    "print(\"✅ Kaggle credentials are set in the notebook.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset\n",
      "✅ Dataset downloaded: /tmp/Bank Customer Churn Prediction.csv\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import glob\n",
    "\n",
    "# Set Kaggle credentials from environment variables\n",
    "if 'KAGGLE_USERNAME' not in os.environ or 'KAGGLE_KEY' not in os.environ:\n",
    "\traise EnvironmentError(\"Kaggle credentials not found in environment variables\")\n",
    "\n",
    "# Initialize Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()  # Uses the environment variable for authentication\n",
    "\n",
    "# Define Kaggle dataset reference\n",
    "dataset_name = \"gauravtopre/bank-customer-churn-dataset\"  # Replace with the actual Kaggle dataset\n",
    "download_path = \"/tmp\"  # Temporary location, not inside your repo\n",
    "\n",
    "# Download dataset (ZIP file)\n",
    "api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n",
    "\n",
    "# Find the downloaded CSV file\n",
    "csv_files = glob.glob(f\"{download_path}/*.csv\")\n",
    "assert len(csv_files) > 0, \"No CSV files found. Check dataset name.\"\n",
    "csv_file_path = csv_files[0]  # Assuming the dataset has a single CSV file\n",
    "\n",
    "print(f\"✅ Dataset downloaded: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset uploaded to BigQuery: kagglebigquerybankchurn.churn_analysis.kaggle_churn\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Define project, dataset, and table details\n",
    "project_id = client.project  # Auto-fetch project ID\n",
    "dataset_id = \"churn_analysis\"  # Choose a dataset name\n",
    "table_id = \"kaggle_churn\"  # Choose a table name\n",
    "full_table_id = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "\n",
    "# Define dataset (if it doesn’t exist)\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "dataset = bigquery.Dataset(dataset_ref)\n",
    "dataset.location = \"US\"  # Adjust region as needed\n",
    "client.create_dataset(dataset, exists_ok=True)\n",
    "\n",
    "# Define table schema (automatically detect types)\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    autodetect=True,  # Automatically infer column types\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,  # Skip the header row\n",
    ")\n",
    "\n",
    "# Load data from CSV to BigQuery table\n",
    "with open(csv_file_path, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, full_table_id, job_config=job_config)\n",
    "\n",
    "# Wait for job completion\n",
    "job.result()\n",
    "\n",
    "print(f\"✅ Dataset uploaded to BigQuery: {full_table_id}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
