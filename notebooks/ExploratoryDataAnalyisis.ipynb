{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Explanation**:  \n",
    "- `os`: Used for handling environment variables and system-level operations.  \n",
    "- `pandas`: Essential for working with data in DataFrames.  \n",
    "- `bigquery`: The **Google Cloud SDK** library for interacting with BigQuery.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **1.2 Setting Up Service Account Credentials ğŸ›¡ï¸**  \n",
    "To connect to BigQuery securely, we store the service account credentials in an **environment variable** instead of hardcoding them in the script.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/workspaces/ABCBankChurnRate/.config/sa_credentials.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Explanation**:  \n",
    "- This ensures that **Google Cloud authentication** is handled securely.  \n",
    "- ğŸ”´ **Question**: *Is this safe enough?* Yes, as long as the `.json` file is properly **gitignored** and its name does not contain sensitive information. \n",
    "\n",
    "---\n",
    "\n",
    "### **1.3 Initializing BigQuery Client & Project Verification ğŸ¢**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "print(client.project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Explanation**:  \n",
    "- `bigquery.Client()` initializes the BigQuery connection.  \n",
    "- `client.project` ensures that authentication was successful by printing the **associated GCP project ID**.  \n",
    "- **Is this necessary?** No, but it's a good debugging step to verify access.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2ï¸âƒ£ Fetching the Dataset from Kaggle ğŸ“¥**  \n",
    "\n",
    "### **2.1 Importing the Kaggle API & Handling Authentication ğŸ”‘**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Explanation**:  \n",
    "- `KaggleApi()`: Enables programmatic access to **Kaggle datasets**.  \n",
    "- `glob`: Useful for searching and handling downloaded files.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 Setting Kaggle Credentials from Environment Variables ğŸ”‘**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set Kaggle credentials from environment variables\n",
    "if 'KAGGLE_USERNAME' not in os.environ or 'KAGGLE_KEY' not in os.environ:\n",
    "\traise EnvironmentError(\"Kaggle credentials not found in environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Explanation**:  \n",
    "- Ensures that the Kaggle credentials are correctly set in the environment.  \n",
    "- Raises an error if the credentials are missing.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2.3 Downloading the Kaggle Dataset ğŸš€**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()  # Uses the environment variable for authentication\n",
    "\n",
    "\n",
    "# Define Kaggle dataset reference\n",
    "dataset_name = \"gauravtopre/bank-customer-churn-dataset\"  # Replace with the actual Kaggle dataset\n",
    "download_path = \"/tmp\"  # Temporary location, not inside this repo\n",
    "\n",
    "# Download dataset (ZIP file)\n",
    "api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n",
    "\n",
    "# Find the downloaded CSV file\n",
    "csv_files = glob.glob(f\"{download_path}/*.csv\")\n",
    "assert len(csv_files) > 0, \"No CSV files found. Check dataset name.\"\n",
    "csv_file_path = csv_files[0]  # Assuming the dataset has a single CSV file\n",
    "\n",
    "print(f\"âœ… Dataset downloaded: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Explanation**:  \n",
    "- **Authenticates the Kaggle API**.  \n",
    "- **Downloads & extracts** the dataset to `/tmp` (a temporary folder, avoiding repo clutter).  \n",
    "- Uses `glob` to locate the downloaded **CSV file**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3ï¸âƒ£ Uploading Dataset to BigQuery ğŸ“¤**  \n",
    "\n",
    "### **3.1 Initializing BigQuery Client ğŸ¯**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize BigQuery client\n",
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Question**: Should the **client variable** have a different name?  \n",
    "âœ”ï¸ Yes! To avoid confusion, we renamed the **BigQuery client** as `bq_client`, since we previously initialized a `client` variable for Kaggle.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” **Step 3.2: Ensuring Dataset and Table Exist in BigQuery**\n",
    "\n",
    "\n",
    "In this step, we ensure that our **BigQuery dataset and table** are correctly set up before running queries. The code performs three key actions:\n",
    "\n",
    "### ğŸ— **1. Initialize BigQuery Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This creates a **client instance** to interact with BigQuery.\n",
    "- It automatically detects the **Google Cloud project** tied to our credentials.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“‚ **2. Define Dataset and Table Details**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset and table details\n",
    "project_id = bq_client.project  # Auto-fetch project ID\n",
    "dataset_id = f\"{project_id}.churn_analysis\"\n",
    "table_id = \"kaggle_churn\"\n",
    "full_table_id = f\"{dataset_id}.{table_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- `project_id` ğŸ¢ â†’ Automatically fetches our **Google Cloud project** name.\n",
    "- `dataset_id` ğŸ“ â†’ Defines the **dataset name** in BigQuery.\n",
    "- `table_id` ğŸ·ï¸ â†’ Specifies the **table name** where data will be stored.\n",
    "- `full_table_id` ğŸ† â†’ Combines the above to form the **full path** to our table.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **3. Ensure the Dataset Exists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Ensure dataset exists\n",
    "dataset_ref = bigquery.Dataset(dataset_id)\n",
    "dataset_ref.location = \"US\"  # Set location (adjust as needed)\n",
    "bq_client.create_dataset(dataset_ref, exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checks if **dataset exists**, creating it if missing.  \n",
    "- We specify `\"US\"` as the **location** (adjust if needed).  \n",
    "- `exists_ok=True` prevents errors if the dataset already exists.  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” **4. Check If the Table Exists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bq_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# âœ… Check if table already exists\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tables = {table.table_id \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbq_client\u001b[49m.list_tables(dataset_id)}\n",
      "\u001b[31mNameError\u001b[39m: name 'bq_client' is not defined"
     ]
    }
   ],
   "source": [
    "# âœ… Check if table already exists\n",
    "tables = {table.table_id for table in bq_client.list_tables(dataset_id)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retrieves **all tables** in our dataset.\n",
    "- Stores their **names in a set** for fast lookup.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ **5. Upload Data if Table is Missing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if table_id not in tables:\n",
    "    print(\"âš ï¸ Table not found. Uploading dataset...\")\n",
    "\n",
    "    # Define schema detection & load configuration\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,  \n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1  # Skip header row\n",
    "    )\n",
    "\n",
    "    # Load data from CSV to BigQuery\n",
    "    with open(csv_file_path, \"rb\") as source_file:\n",
    "        job = bq_client.load_table_from_file(source_file, full_table_id, job_config=job_config)\n",
    "    \n",
    "    # Wait for job completion\n",
    "    job.result()\n",
    "    print(f\"âœ… Dataset uploaded to BigQuery: {full_table_id}\")\n",
    "else:\n",
    "    print(f\"âœ… Table '{table_id}' already exists in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **If the table doesnâ€™t exist**, it uploads the dataset:\n",
    "  - Uses `autodetect=True` ğŸ“Š â†’ Automatically detects **column types**.\n",
    "  - Reads the **CSV file** and uploads it to BigQuery.\n",
    "  - **`job.result()`** waits until upload finishes.\n",
    "  - Finally, it **confirms successful upload** âœ….\n",
    "\n",
    "- **If the table already exists**, it simply prints a confirmation âœ….\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ **Key Takeaways**\n",
    "âœ… Ensures **dataset exists** before proceeding  \n",
    "âœ… Checks for **existing tables** to avoid duplicate uploads  \n",
    "âœ… Uses **schema autodetection** to simplify setup  \n",
    "âœ… **Uploads data only if missing**, making the process efficient  \n",
    "\n",
    "This ensures that our **BigQuery setup is reliable** before running queries! ğŸš€  \n",
    "\n",
    "## **4ï¸âƒ£ Querying BigQuery Data Using SQL & Python ğŸ”**  \n",
    "\n",
    "### **4.1 Running an Example Query: Fetching First 10 Rows ğŸ“‹**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM `kagglebigquerybankchurn.churn_analysis.kaggle_churn`\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# Run query\n",
    "query_job = client.query(query)\n",
    "df = query_job.to_dataframe()\n",
    "\n",
    "# Display results\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ğŸ“Œ **Explanation**:  \n",
    "- Runs a **basic SQL query** to fetch the first 10 rows.  \n",
    "- Converts the results into a Pandas **DataFrame** for easy exploration.  \n",
    "\n",
    "---\n",
    "\n",
    "## **5ï¸âƒ£ Defining & Organizing Reusable Queries ğŸ“‘**  \n",
    "\n",
    "### **5.1 Structuring SQL Queries as Named Variables ğŸ·ï¸**  \n",
    "\n",
    "Here, we will create two variables that store SQL queries. This allows us to easily reference and reuse them throughout the notebook.\n",
    "\n",
    "The first one, `query_avg_credit_score` assess oveall customer quality. Credit scores are widely used in the financial industry to assess customer reliability and financial health. An example of using credit metrics in customer anlysis available [here](https://www.researchgate.net/publication/357002993_Customer_churn_prediction_for_telecommunication_industry_A_Malaysian_Case_Study).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_avg_credit_score = \"\"\"\n",
    "SELECT AVG(credit_score) AS avg_credit_score\n",
    "FROM `kagglebigquerybankchurn.churn_analysis.kaggle_churn`\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe will only have one value, so it has limited information about how churn rate is related to credit score itself. for that, we can do another query,`churn_rate_by_credit_score`, that will give us a better idea of how credit score is related to churn rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_rate_by_credit_score = \"\"\"\" \\\n",
    "SELECT credit_score, COUNT(*) AS count\" \\\" \\\n",
    "FROM `kagglebigquerybankchurn.churn_analysis.kaggle_churn`\" \\\n",
    "GROUP BY credit_score\" \\\" \\\n",
    "ORDER BY credit_score\" \\\" \\\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "# Run query\n",
    "query_job = client.query(churn_rate_by_credit_score)\n",
    "df_churn_rate_by_credit_score = query_job.to_dataframe()\n",
    "# Display results\n",
    "df_churn_rate_by_credit_score.head()\n",
    "# Display average credit score\n",
    "query_job = client.query(query_avg_credit_score)\n",
    "df_avg_credit_score = query_job.to_dataframe()\n",
    "# Display results\n",
    "df_avg_credit_score.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next query `query_churn_by_country` will show that variations in churn by country may reveal that regional economic conditions, local competition, or cultural factors affect customer behavior. This insight supports including geographical segmentation in the models we will build. It also will allow us to choose whether or not new features should be engineered based on the country of the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_churn_by_country = \"\"\"\n",
    "SELECT country, COUNT(*) AS churn_count\n",
    "FROM `kagglebigquerybankchurn.churn_analysis.kaggle_churn`\n",
    "WHERE churn = 1\n",
    "GROUP BY country\n",
    "ORDER BY churn_count DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ğŸ“Œ **Best Practice**:  \n",
    "âœ”ï¸ Each query is stored in a clearly named **variable** for reusability.  \n",
    "âœ”ï¸ Queries are formatted and documented for better readability.  \n",
    "\n",
    "---\n",
    "\n",
    "### **5.2 Running & Storing Queries in Named DataFrames ğŸ“Š**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query, query_name):\n",
    "    query_job = bq_client.query(query)\n",
    "    df = query_job.to_dataframe()\n",
    "    print(f\"âœ… Query executed: {query_name}\")\n",
    "    return df\n",
    "\n",
    "df_avg_credit_score = run_query(query_avg_credit_score, \"Average Credit Score\")\n",
    "df_churn_by_country = run_query(query_churn_by_country, \"Churn Count by Country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's examine these dataframes to understand the data we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_credit_score.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since the average credit score is a sample estimator of the real population parameter, we can rather visualize the distribution of the variable, and visualize it's relationship with the churn rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ“Œ **Best Practice**:  \n",
    "âœ”ï¸ Encapsulates query execution inside a **function** for efficiency.  \n",
    "âœ”ï¸ Names each DataFrame meaningfully for easy reference.  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **6ï¸âƒ£ Next Steps & Production Considerations ğŸš€** \n",
    "\n",
    "\n",
    "### **ğŸ¯ Enhancing SQL Queries**\n",
    "ğŸ”¹ Identify **key features** correlated with churn.  \n",
    "ğŸ”¹ Apply **window functions, joins, and aggregations** for advanced insights.  \n",
    "ğŸ”¹ Perform **data transformations** inside BigQuery before fetching results.  \n",
    "\n",
    "### **ğŸ“ˆ Visualization & EDA**\n",
    "ğŸ”¹ Use **matplotlib & seaborn** to visualize churn distribution.  \n",
    "ğŸ”¹ Compare **churned vs. retained customers** across different variables.  \n",
    "\n",
    "### **ğŸ”„ CI/CD & Deployment**\n",
    "ğŸ”¹ Automate query execution using **GitHub Actions**.  \n",
    "ğŸ”¹ Integrate results into a **dashboard or API**.  \n",
    "\n",
    "---\n",
    "\n",
    "This structured approach ensures that our notebook remains **organized, efficient, and production-ready**. ğŸš€ğŸ”  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
