{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Customer Churn Analysis with BigQuery & Python 🚀\n",
    "\n",
    "## **Introduction**\n",
    "This notebook is part of this **customer churn prediction project**, where we analyze bank customer data using **Google BigQuery** and **Python**. Our goal is to identify key factors influencing customer retention and to build a predictive model based on our findings.\n",
    "\n",
    "## **🔗 Key Milestones Achieved So Far**\n",
    "✅ **Cloud Integration**: We set up a **Google Cloud Service Account** to securely connect Kaggle datasets with BigQuery.  \n",
    "✅ **Database Setup**: The dataset was uploaded to **BigQuery** for efficient querying and analysis.  \n",
    "✅ **Local Execution**: Instead of downloading large files, we now run **SQL queries directly from this notebook** and retrieve data as Pandas DataFrames.  \n",
    "\n",
    "## **📌 Current Focus**\n",
    "🔹 Writing and executing **SQL queries** in BigQuery to explore churn-related patterns.  \n",
    "🔹 Using **Python & Pandas** to analyze query results and create **visualizations**.  \n",
    "🔹 Investigating feature selection techniques to identify the most relevant variables for churn prediction.  \n",
    "\n",
    "## **🚀 Next Steps & Production Plan**\n",
    "🔜 Transitioning from exploratory analysis to **machine learning modeling**.  \n",
    "🔜 Designing a **CI/CD pipeline** for automating data ingestion and model updates.  \n",
    "🔜 Deploying insights into a **dashboard or API** for real-time churn monitoring.  \n",
    "\n",
    "Let’s dive into the data and extract meaningful insights! 🔍📈"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📘 **Notebook Structure & Documentation** 🏗️  \n",
    "\n",
    "To ensure clarity and maintainability, this notebook follows a structured approach, separating each key process into well-defined sections. Each section includes a detailed explanation of the code, making it easier to understand and extend.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1️⃣ Setting Up the Environment & Authentication 🔐**\n",
    "\n",
    "### **1.1 Importing Required Libraries 📦**  \n",
    "Before interacting with **Google BigQuery** and **Kaggle**, we need to import the necessary Python libraries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 **Explanation**:  \n",
    "- `os`: Used for handling environment variables and system-level operations.  \n",
    "- `pandas`: Essential for working with data in DataFrames.  \n",
    "- `bigquery`: The **Google Cloud SDK** library for interacting with BigQuery.  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **1.2 Setting Up Service Account Credentials 🛡️**  \n",
    "To connect to BigQuery securely, we store the service account credentials in an **environment variable** instead of hardcoding them in the script.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/workspaces/ABCBankChurnRate/.config/sa_credentials.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 **Explanation**:  \n",
    "- This ensures that **Google Cloud authentication** is handled securely.  \n",
    "- 🔴 **Question**: *Is this safe enough?* Yes, as long as the `.json` file is properly **gitignored** and its name does not contain sensitive information. \n",
    "\n",
    "---\n",
    "\n",
    "### **1.3 Initializing BigQuery Client & Project Verification 🏢**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kagglebigquerybankchurn\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client()\n",
    "print(client.project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 **Explanation**:  \n",
    "- `bigquery.Client()` initializes the BigQuery connection.  \n",
    "- `client.project` ensures that authentication was successful by printing the **associated GCP project ID**.  \n",
    "- **Is this necessary?** No, but it's a good debugging step to verify access.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2️⃣ Fetching the Dataset from Kaggle 📥**  \n",
    "\n",
    "### **2.1 Importing the Kaggle API & Handling Authentication 🔑**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 **Explanation**:  \n",
    "- `KaggleApi()`: Enables programmatic access to **Kaggle datasets**.  \n",
    "- `glob`: Useful for searching and handling downloaded files.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 Setting Kaggle Credentials from Environment Variables 🔑**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set Kaggle credentials from environment variables\n",
    "if 'KAGGLE_USERNAME' not in os.environ or 'KAGGLE_KEY' not in os.environ:\n",
    "\traise EnvironmentError(\"Kaggle credentials not found in environment variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 **Explanation**:  \n",
    "- Ensures that the Kaggle credentials are correctly set in the environment.  \n",
    "- Raises an error if the credentials are missing.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2.3 Downloading the Kaggle Dataset 🚀**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset\n",
      "✅ Dataset downloaded: /tmp/Bank Customer Churn Prediction.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()  # Uses the environment variable for authentication\n",
    "\n",
    "\n",
    "# Define Kaggle dataset reference\n",
    "dataset_name = \"gauravtopre/bank-customer-churn-dataset\"  # Replace with the actual Kaggle dataset\n",
    "download_path = \"/tmp\"  # Temporary location, not inside this repo\n",
    "\n",
    "# Download dataset (ZIP file)\n",
    "api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n",
    "\n",
    "# Find the downloaded CSV file\n",
    "csv_files = glob.glob(f\"{download_path}/*.csv\")\n",
    "assert len(csv_files) > 0, \"No CSV files found. Check dataset name.\"\n",
    "csv_file_path = csv_files[0]  # Assuming the dataset has a single CSV file\n",
    "\n",
    "print(f\"✅ Dataset downloaded: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 **Explanation**:  \n",
    "- **Authenticates the Kaggle API**.  \n",
    "- **Downloads & extracts** the dataset to `/tmp` (a temporary folder, avoiding repo clutter).  \n",
    "- Uses `glob` to locate the downloaded **CSV file**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3️⃣ Uploading Dataset to BigQuery 📤**  \n",
    "\n",
    "### **3.1 Initializing BigQuery Client 🎯**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Initialize BigQuery client\n",
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 **Question**: Should the **client variable** have a different name?  \n",
    "✔️ Yes! To avoid confusion, we renamed the **BigQuery client** as `bq_client`, since we previously initialized a `client` variable for Kaggle.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 **Step 3.2: Ensuring Dataset and Table Exist in BigQuery**\n",
    "\n",
    "\n",
    "In this step, we ensure that our **BigQuery dataset and table** are correctly set up before running queries. The code performs three key actions:\n",
    "\n",
    "### 🏗 **1. Initialize BigQuery Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bq_client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This creates a **client instance** to interact with BigQuery.\n",
    "- It automatically detects the **Google Cloud project** tied to our credentials.\n",
    "\n",
    "---\n",
    "\n",
    "### 📂 **2. Define Dataset and Table Details**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset and table details\n",
    "project_id = bq_client.project  # Auto-fetch project ID\n",
    "dataset_id = f\"{project_id}.churn_analysis\"\n",
    "table_id = \"kaggle_churn\"\n",
    "full_table_id = f\"{dataset_id}.{table_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- `project_id` 🏢 → Automatically fetches our **Google Cloud project** name.\n",
    "- `dataset_id` 📁 → Defines the **dataset name** in BigQuery.\n",
    "- `table_id` 🏷️ → Specifies the **table name** where data will be stored.\n",
    "- `full_table_id` 🏆 → Combines the above to form the **full path** to our table.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **3. Ensure the Dataset Exists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Ensure dataset exists\n",
    "dataset_ref = bigquery.Dataset(dataset_id)\n",
    "dataset_ref.location = \"US\"  # Set location (adjust as needed)\n",
    "bq_client.create_dataset(dataset_ref, exists_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Checks if **dataset exists**, creating it if missing.  \n",
    "- We specify `\"US\"` as the **location** (adjust if needed).  \n",
    "- `exists_ok=True` prevents errors if the dataset already exists.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 **4. Check If the Table Exists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Check if table already exists\n",
    "tables = {table.table_id for table in bq_client.list_tables(dataset_id)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retrieves **all tables** in our dataset.\n",
    "- Stores their **names in a set** for fast lookup.\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 **5. Upload Data if Table is Missing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Table 'kaggle_churn' already exists in dataset.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if table_id not in tables:\n",
    "    print(\"⚠️ Table not found. Uploading dataset...\")\n",
    "\n",
    "    # Define schema detection & load configuration\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        autodetect=True,  \n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "        skip_leading_rows=1  # Skip header row\n",
    "    )\n",
    "\n",
    "    # Load data from CSV to BigQuery\n",
    "    with open(csv_file_path, \"rb\") as source_file:\n",
    "        job = bq_client.load_table_from_file(source_file, full_table_id, job_config=job_config)\n",
    "    \n",
    "    # Wait for job completion\n",
    "    job.result()\n",
    "    print(f\"✅ Dataset uploaded to BigQuery: {full_table_id}\")\n",
    "else:\n",
    "    print(f\"✅ Table '{table_id}' already exists in dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **If the table doesn’t exist**, it uploads the dataset:\n",
    "  - Uses `autodetect=True` 📊 → Automatically detects **column types**.\n",
    "  - Reads the **CSV file** and uploads it to BigQuery.\n",
    "  - **`job.result()`** waits until upload finishes.\n",
    "  - Finally, it **confirms successful upload** ✅.\n",
    "\n",
    "- **If the table already exists**, it simply prints a confirmation ✅.\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **Key Takeaways**\n",
    "✅ Ensures **dataset exists** before proceeding  \n",
    "✅ Checks for **existing tables** to avoid duplicate uploads  \n",
    "✅ Uses **schema autodetection** to simplify setup  \n",
    "✅ **Uploads data only if missing**, making the process efficient  \n",
    "\n",
    "This ensures that our **BigQuery setup is reliable** before running queries! 🚀  \n",
    "\n",
    "## **4️⃣ Querying BigQuery Data Using SQL & Python 🔎**  \n",
    "\n",
    "### **4.1 Running an Example Query: Fetching First 10 Rows 📋**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>products_number</th>\n",
       "      <th>credit_card</th>\n",
       "      <th>active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15647091</td>\n",
       "      <td>725</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>75888.20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45613.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15713826</td>\n",
       "      <td>613</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>117356.19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113557.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15633840</td>\n",
       "      <td>781</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>125023.10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>108301.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15769915</td>\n",
       "      <td>643</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>133313.34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3965.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15652674</td>\n",
       "      <td>539</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>83459.86</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>146752.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  credit_score  country  gender  age  tenure    balance  \\\n",
       "0     15647091           725  Germany    Male   19       0   75888.20   \n",
       "1     15713826           613  Germany  Female   20       0  117356.19   \n",
       "2     15633840           781   France    Male   20       0  125023.10   \n",
       "3     15769915           643    Spain  Female   20       0  133313.34   \n",
       "4     15652674           539   France    Male   20       0   83459.86   \n",
       "\n",
       "   products_number  credit_card  active_member  estimated_salary  churn  \n",
       "0                1            0              0          45613.75      0  \n",
       "1                1            0              0         113557.70      1  \n",
       "2                2            1              1         108301.45      0  \n",
       "3                1            1              1           3965.69      0  \n",
       "4                1            1              1         146752.67      0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT * FROM `kagglebigquerybankchurn.churn_analysis.kaggle_churn`\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "\n",
    "# Run query\n",
    "query_job = client.query(query)\n",
    "df = query_job.to_dataframe()\n",
    "\n",
    "# Display results\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "📌 **Explanation**:  \n",
    "- Runs a **basic SQL query** to fetch the first 10 rows.  \n",
    "- Converts the results into a Pandas **DataFrame** for easy exploration.  \n",
    "\n",
    "---\n",
    "\n",
    "## **5️⃣ Defining & Organizing Reusable Queries 📑**  \n",
    "\n",
    "### **5.1 Structuring SQL Queries as Named Variables 🏷️**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_avg_credit_score = \"\"\"\n",
    "SELECT AVG(credit_score) AS avg_credit_score\n",
    "FROM `kagglebigquerybankchurn.churn_analysis.kaggle_churn`\n",
    "\"\"\"\n",
    "query_churn_by_country = \"\"\"\n",
    "SELECT country, COUNT(*) AS churn_count\n",
    "FROM `kagglebigquerybankchurn.churn_analysis.kaggle_churn`\n",
    "WHERE churn = 1\n",
    "GROUP BY country\n",
    "ORDER BY churn_count DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "📌 **Best Practice**:  \n",
    "✔️ Each query is stored in a clearly named **variable** for reusability.  \n",
    "✔️ Queries are formatted and documented for better readability.  \n",
    "\n",
    "---\n",
    "\n",
    "### **5.2 Running & Storing Queries in Named DataFrames 📊**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Query executed: Average Credit Score\n",
      "✅ Query executed: Churn Count by Country\n"
     ]
    }
   ],
   "source": [
    "def run_query(query, query_name):\n",
    "    query_job = bq_client.query(query)\n",
    "    df = query_job.to_dataframe()\n",
    "    print(f\"✅ Query executed: {query_name}\")\n",
    "    return df\n",
    "\n",
    "df_avg_credit_score = run_query(query_avg_credit_score, \"Average Credit Score\")\n",
    "df_churn_by_country = run_query(query_churn_by_country, \"Churn Count by Country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "📌 **Best Practice**:  \n",
    "✔️ Encapsulates query execution inside a **function** for efficiency.  \n",
    "✔️ Names each DataFrame meaningfully for easy reference.  \n",
    "\n",
    "---\n",
    "\n",
    "## **6️⃣ Next Steps & Production Considerations 🚀** \n",
    "\n",
    "\n",
    "### **🎯 Enhancing SQL Queries**\n",
    "🔹 Identify **key features** correlated with churn.  \n",
    "🔹 Apply **window functions, joins, and aggregations** for advanced insights.  \n",
    "🔹 Perform **data transformations** inside BigQuery before fetching results.  \n",
    "\n",
    "### **📈 Visualization & EDA**\n",
    "🔹 Use **matplotlib & seaborn** to visualize churn distribution.  \n",
    "🔹 Compare **churned vs. retained customers** across different variables.  \n",
    "\n",
    "### **🔄 CI/CD & Deployment**\n",
    "🔹 Automate query execution using **GitHub Actions**.  \n",
    "🔹 Integrate results into a **dashboard or API**.  \n",
    "\n",
    "---\n",
    "\n",
    "This structured approach ensures that our notebook remains **organized, efficient, and production-ready**. 🚀🔍  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
