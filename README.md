# Bank Customer Churn Prediction

A production-ready machine learning project to predict customer churn in a banking context, comparing multiple algorithms including *## ğŸ“‹ Dependencies

Core packages:
- `scikit-learn## ğŸ Quick Results

After running `python demo.py`, you'll get results from **real bank customer data**:

**With Kaggle credentials:**
- ğŸ“Š **Real data analysis** from 10,000 customer records
- ğŸ† **XGBoost achieves 84.8% F1-score** on real data
- ğŸ“ˆ **Performance improvements** up to +14.5% over synthetic data

**Without Kaggle credentials:**
- ğŸ”„ **Automatic fallback** to synthetic data (2,000 samples)
- âš¡ **Still functional** for testing and demonstration
- ğŸ“‹ **Same analysis pipeline** and reporting structure

Check the `demo_results/` folder for:
- ğŸ“„ Executive summary for business stakeholders
- ğŸ“Š Performance comparison charts (real vs synthetic if applicable)
- ğŸ“ˆ Feature importance analysis
- ğŸ“‹ Technical metrics report

**Plus comprehensive visualizations in `visualizations/` folder:**
- ğŸ” Exploratory data analysis plots
- ğŸ“Š Model performance comparisons
- ğŸ¯ ROC curves and performance metrics
- ğŸ”— Feature correlation heatmaps
- âš¡ Training time vs performance analysisitional ML algorithms
- `xgboost`: Gradient boosting
- `pandas`: Data manipulation
- **`kaggle`**: Real dataset API integration â­
- `google-cloud-bigquery`: Cloud data warehouse
- `matplotlib`, `seaborn`: Data visualization(Tabular Model), **XGBoost**, **Random Forest**, **Logistic Regression**, **Naive Bayes**, and **AdaBoost**.

## ğŸš€ Quick Start

### Demo Mode (Real Kaggle Data)
```bash
git clone <repository-url>
cd ABCBankChurnRate
pip install -r requirements.txt

# Set Kaggle credentials (if available)
export KAGGLE_JSON='{"username":"your_username","key":"your_key"}'

# Run demo with real data
python demo.py
```

**The demo automatically:**
- ğŸ”„ **Downloads real data** from [Kaggle Bank Customer Churn Dataset](https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset) using Kaggle API
- ğŸ“Š **Trains all 6 ML models** on 10,000 real customer records
- ğŸ’¾ **Saves data locally** for future runs
- ğŸ”„ **Falls back to synthetic data** if Kaggle credentials unavailable

### Production Mode (With BigQuery Storage)
```bash
# Set environment variables
export KAGGLE_JSON='{"username":"your_username","key":"your_key"}'
export GCP_SA_KEY='{"type":"service_account",...}'

# Run full pipeline with BigQuery integration
python main.py --download-new
```

## ğŸ“Š Key Results (Real Data)

| Model | F1-Score | ROC-AUC | Training Time |
|-------|----------|---------|---------------|
| **XGBoost** | **0.848** | 0.857 | 0.18s |
| TabM | 0.843 | 0.856 | 1.25s |
| RandomForest | 0.843 | 0.859 | 0.62s |
| AdaBoost | 0.834 | 0.843 | 0.19s |
| NaiveBayes | 0.804 | 0.797 | 0.003s |
| LogisticRegression | 0.769 | 0.778 | 0.01s |

*Results from 10,000 real customer records with 20.37% churn rate*

### ğŸ“Š Churn Distribution Analysis
![Churn Distribution](visualizations/churn_distribution.png)

### ğŸ‘¥ Demographic Insights  
![Demographic Analysis](visualizations/demographic_analysis.png)

### ğŸ’° Financial Behavior Patterns
![Financial Analysis](visualizations/financial_analysis.png)

## ğŸ“ˆ Real vs Synthetic Data Performance Comparison

| Model | F1-Score (Real) | F1-Score (Synthetic) | Improvement |
|-------|-----------------|---------------------|-------------|
| **XGBoost** | **0.848** | 0.749 | **+13.2%** |
| RandomForest | 0.843 | 0.736 | **+14.5%** |
| TabM | 0.843 | 0.736 | **+14.5%** |
| AdaBoost | 0.834 | 0.747 | **+11.6%** |
| NaiveBayes | 0.804 | 0.762 | **+5.5%** |
| LogisticRegression | 0.769 | 0.748 | **+2.8%** |

**Key Insights:**
- ğŸ¯ **Real data consistently outperforms synthetic data** across all models
- ğŸ“Š **Average improvement: +10.4%** in F1-score
- ğŸš€ **Ensemble methods benefit most** from real-world patterns
- âš¡ **5x more training data** (10,000 vs 2,000 samples) enables better learning

### ğŸ¯ Model Performance Comparison
![Model Comparison](visualizations/model_comparison.png)

### âš¡ Performance vs Training Time Analysis
![Time vs Performance](visualizations/time_vs_performance.png)

### ğŸ“ˆ ROC Curve Analysis
![ROC Curves](visualizations/roc_curves.png)

## ğŸ—ï¸ Project Structure

```
ABCBankChurnRate/
â”œâ”€â”€ ğŸ“ config/              # Configuration management
â”‚   â””â”€â”€ config.py           # Project settings
â”œâ”€â”€ ğŸ“ notebooks/           # Jupyter notebooks for exploration
â”‚   â”œâ”€â”€ ExploratoryDA.ipynb
â”‚   â””â”€â”€ Modelling.ipynb
â”œâ”€â”€ ğŸ“ src/                 # Source code
â”‚   â”œâ”€â”€ ğŸ“ data/           # Data handling
â”‚   â”‚   â”œâ”€â”€ data_loader.py  # Kaggle & BigQuery integration
â”‚   â”‚   â””â”€â”€ preprocessor.py # Feature engineering
â”‚   â”œâ”€â”€ ï¿½ models/         # ML model implementations
â”‚   â”‚   â”œâ”€â”€ base_model.py   # Abstract base class
â”‚   â”‚   â”œâ”€â”€ logistic_regression.py
â”‚   â”‚   â”œâ”€â”€ naive_bayes.py
â”‚   â”‚   â”œâ”€â”€ random_forest.py
â”‚   â”‚   â”œâ”€â”€ xgboost_model.py
â”‚   â”‚   â”œâ”€â”€ adaboost.py
â”‚   â”‚   â”œâ”€â”€ tabm.py         # TabM implementation
â”‚   â”‚   â””â”€â”€ trainer.py      # Training pipeline
â”‚   â””â”€â”€ ğŸ“ utils/          # Utilities
â”‚       â”œâ”€â”€ reporting.py    # Stakeholder reports
â”‚       â””â”€â”€ visualizations.py # Chart generation
â”œâ”€â”€ ğŸ“ tests/              # Unit tests
â”œâ”€â”€ ğŸ“ visualizations/     # Generated plots & charts
â”œâ”€â”€ demo.py                # Quick demo script
â”œâ”€â”€ generate_visualizations.py # Visualization generator
â”œâ”€â”€ hyperparameter_tuning_demo.py # HP tuning demo
â”œâ”€â”€ main.py                # Production pipeline
â””â”€â”€ requirements.txt       # Dependencies
```

## ğŸ”§ Features

### ğŸ¯ Kaggle API Integration â­
- **Automatic Download**: Fetches real bank customer data from Kaggle
- **Smart Caching**: Saves data locally to avoid repeated downloads
- **Credential Management**: Secure handling of Kaggle API keys
- **Graceful Fallback**: Uses synthetic data if Kaggle unavailable
- **Production Ready**: Same approach used in EDA notebooks

### Data Pipeline
- âœ… **Kaggle API Integration**: Automatic download of real bank customer churn dataset (10,000 records)
- âœ… **BigQuery Storage**: Scalable cloud data warehouse for production
- âœ… **Smart Fallback**: Uses synthetic data if Kaggle credentials unavailable
- âœ… **Data Validation**: Comprehensive duplicate and missing value checks
- âœ… **Feature Engineering**: Automated preprocessing pipeline

### Machine Learning
- âœ… **6 ML Algorithms**: Including advanced TabM model
- âœ… **Hyperparameter Tuning**: Grid search with cross-validation
- âœ… **Overfitting Prevention**: Stratified splits and validation
- âœ… **Model Persistence**: Automatic pickle saving/loading

### Production Ready
- âœ… **Comprehensive Testing**: Unit and integration tests
- âœ… **Stakeholder Reports**: Executive summaries and technical reports
- âœ… **Visualization**: Performance comparison plots
- âœ… **Error Handling**: Robust error management
- âœ… **Documentation**: Complete API documentation

### ğŸ“Š Comprehensive Visualizations
- âœ… **EDA Visualizations**: Churn distribution, demographics, financial patterns
- âœ… **Correlation Analysis**: Feature relationships and multicollinearity detection
- âœ… **Model Performance**: Comparison charts, ROC curves, performance radar
- âœ… **Hyperparameter Analysis**: Parameter impact visualization
- âœ… **Time vs Performance**: Training efficiency analysis

## ğŸ“ˆ Model Comparison

### Traditional ML Models
- **Logistic Regression**: Fast, interpretable baseline
- **Naive Bayes**: Probabilistic approach
- **Random Forest**: Ensemble method with feature importance
- **AdaBoost**: Adaptive boosting classifier
- **XGBoost**: Gradient boosting with advanced optimization

### Advanced Models
- **TabM**: Neural network optimized for tabular data
  - Multi-layer perceptron with proper scaling
  - Adaptive learning rates
  - Early stopping to prevent overfitting

## ğŸ” Data Analysis & Feature Engineering

### ğŸ”— Feature Correlation Analysis
![Correlation Heatmap](visualizations/correlation_heatmap.png)

### ğŸ“Š Feature Distribution Analysis
![Feature Distributions](visualizations/feature_distributions.png)

### ğŸ›ï¸ Hyperparameter Tuning Insights
![Hyperparameter Analysis](visualizations/hyperparameter_analysis.png)

## ğŸ¯ Business Impact

### Why Keep Synthetic Data?
- ğŸ”“ **Accessibility**: Demo works without Kaggle account setup
- ğŸ§ª **Testing**: Reliable fallback for CI/CD pipelines
- ğŸ“š **Education**: Shows algorithm behavior on controlled data
- ğŸ”’ **Privacy**: No external API dependencies for basic testing

### Metrics Explained
- **F1-Score**: Balanced measure of precision and recall
- **ROC-AUC**: Model's ability to distinguish between classes
- **Precision**: % of predicted churners who actually churn
- **Recall**: % of actual churners correctly identified

### Stakeholder Deliverables
1. **Executive Summary** (`demo_results/executive_summary.md`)
2. **Technical Report** (`demo_results/detailed_technical_report.csv`)
3. **Visualization Package** (PNG files for presentations)
4. **Feature Importance Analysis** (Key churn drivers)

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest tests/ -v

# Run specific test
python -m pytest tests/test_pipeline.py -v

# Run with coverage
python -m pytest tests/ --cov=src
```

## ğŸ“Š Generate Visualizations

```bash
# Generate comprehensive EDA and model visualizations
python generate_visualizations.py

# Hyperparameter tuning with visualizations
python hyperparameter_tuning_demo.py
```

**Generated visualizations include:**
- ğŸ” **EDA plots**: Churn distribution, demographics, financial patterns
- ğŸ“Š **Model comparisons**: Performance metrics, ROC curves
- ğŸ¯ **Training analysis**: Time vs performance, hyperparameter impact
- ğŸ”— **Feature analysis**: Correlations, distributions

## âš™ï¸ Configuration

Edit `config/config.py` to customize:
- Feature engineering strategies
- Model hyperparameters
- Data processing settings
- File paths and BigQuery settings

## ï¿½ Dependencies

Core packages:
- `scikit-learn`: Traditional ML algorithms
- `xgboost`: Gradient boosting
- `pandas`: Data manipulation
- `google-cloud-bigquery`: Cloud data warehouse
- `kaggle`: Data source API

## ğŸ”„ Workflow

### Data Flow
1. **Data Acquisition**: 
   - **Primary**: Kaggle API â†’ Real bank customer data (10,000 records) â†’ Local cache
   - **Fallback**: Generate synthetic data if Kaggle unavailable
   - **Production**: Kaggle â†’ Local â†’ BigQuery for scalable storage
2. **Validation**: Duplicates, missing values, outliers detection
3. **Preprocessing**: Scaling, encoding, feature engineering
4. **Training**: 6 models with hyperparameter tuning
5. **Evaluation**: Cross-validation and test set metrics
6. **Reporting**: Automated stakeholder deliverables

### Model Training
1. **Data Split**: 60% train, 20% validation, 20% test
2. **Hyperparameter Tuning**: Grid search with 5-fold CV (production mode)
3. **Model Training**: Best parameters on full training set
4. **Evaluation**: Performance on held-out test set
5. **Persistence**: Save models for future use

### Data Sources
- **Real Data**: [Kaggle Bank Customer Churn Dataset](https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset)
  - 10,000 customer records
  - 20.37% churn rate
  - 11 features (credit_score, age, balance, etc.)
- **Synthetic Data**: Generated fallback with realistic banking patterns

## ğŸ Quick Results

After running `python demo.py`, check the `demo_results/` folder for:
- ï¿½ Executive summary for business stakeholders
- ğŸ“Š Performance comparison charts
- ğŸ“ˆ Feature importance analysis
- ğŸ“‹ Technical metrics report

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Run tests: `python -m pytest tests/`
4. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

---

**Built with â¤ï¸ for production ML workflows**

