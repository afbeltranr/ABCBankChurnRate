"""Reporting utilities for stakeholder communication."""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

class StakeholderReporter:
    """Generate reports for stakeholders."""
    
    def __init__(self, results_dir: str):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
    
    def generate_executive_summary(
        self,
        results: Dict[str, Any],
        best_model_name: str,
        dataset_info: Dict[str, Any]
    ) -> str:
        """
        Generate an executive summary for stakeholders.
        
        Args:
            results: Model comparison results
            best_model_name: Name of the best performing model
            dataset_info: Information about the dataset
        
        Returns:
            Executive summary as string
        """
        best_model_results = results[best_model_name]
        test_metrics = best_model_results['test_metrics']
        
        summary = f"""
# CUSTOMER CHURN PREDICTION - EXECUTIVE SUMMARY
Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## KEY FINDINGS

### Best Performing Model: {best_model_name}
- **Accuracy**: {test_metrics['accuracy']:.1%} of predictions are correct
- **Precision**: {test_metrics['precision']:.1%} of predicted churners actually churn
- **Recall**: {test_metrics['recall']:.1%} of actual churners are identified
- **F1-Score**: {test_metrics['f1_score']:.3f} (balanced performance metric)
- **ROC-AUC**: {test_metrics['roc_auc']:.3f} (model discrimination ability)

### Dataset Overview
- **Total Customers**: {dataset_info.get('total_customers', 'N/A'):,}
- **Churn Rate**: {dataset_info.get('churn_rate', 'N/A'):.1%}
- **Features Used**: {dataset_info.get('num_features', 'N/A')}

## BUSINESS IMPACT

### What This Means for the Business:
1. **Accuracy of {test_metrics['accuracy']:.1%}** means the model correctly identifies customer behavior in approximately {test_metrics['accuracy']:.0%} of cases
2. **Precision of {test_metrics['precision']:.1%}** means that when the model predicts a customer will churn, it's correct {test_metrics['precision']:.0%} of the time
3. **Recall of {test_metrics['recall']:.1%}** means the model catches {test_metrics['recall']:.0%} of customers who actually churn

### Recommended Actions:
1. **Implement the {best_model_name} model** for churn prediction
2. **Focus retention efforts** on customers identified as high-risk
3. **Monitor model performance** monthly and retrain quarterly
4. **Investigate feature importance** to understand key churn drivers

## MODEL COMPARISON

The following models were evaluated:

"""
        
        # Add model comparison table
        for model_name, result in results.items():
            metrics = result['test_metrics']
            summary += f"- **{model_name}**: F1-Score {metrics['f1_score']:.3f}, ROC-AUC {metrics['roc_auc']:.3f}\n"
        
        summary += f"""
## NEXT STEPS

1. **Deployment**: Integrate the {best_model_name} model into the customer management system
2. **Monitoring**: Set up automated model performance tracking
3. **Action Planning**: Develop targeted retention strategies based on model predictions
4. **Continuous Improvement**: Schedule regular model retraining with new data

## TECHNICAL NOTES

- All models were trained using {dataset_info.get('training_samples', 'N/A')} training samples
- Cross-validation was used to prevent overfitting
- Hyperparameter tuning was performed to optimize performance
- Models are saved and can be deployed immediately

---
*Report generated by the Customer Churn Prediction System*
"""
        
        return summary
    
    def save_executive_summary(
        self,
        results: Dict[str, Any],
        best_model_name: str,
        dataset_info: Dict[str, Any]
    ) -> str:
        """Save executive summary to file."""
        summary = self.generate_executive_summary(results, best_model_name, dataset_info)
        
        summary_path = self.results_dir / "executive_summary.md"
        with open(summary_path, 'w') as f:
            f.write(summary)
        
        return str(summary_path)
    
    def generate_detailed_report(self, results: Dict[str, Any]) -> pd.DataFrame:
        """Generate detailed technical report."""
        detailed_data = []
        
        for model_name, result in results.items():
            test_metrics = result['test_metrics']
            val_metrics = result['validation_metrics']
            
            detailed_data.append({
                'Model': model_name,
                'Test_Accuracy': f"{test_metrics['accuracy']:.4f}",
                'Test_Precision': f"{test_metrics['precision']:.4f}",
                'Test_Recall': f"{test_metrics['recall']:.4f}",
                'Test_F1_Score': f"{test_metrics['f1_score']:.4f}",
                'Test_ROC_AUC': f"{test_metrics['roc_auc']:.4f}",
                'Val_Accuracy': f"{val_metrics['accuracy']:.4f}",
                'Val_F1_Score': f"{val_metrics['f1_score']:.4f}",
                'Training_Time_Seconds': f"{result['training_time']:.2f}",
                'Overfit_Risk': self._assess_overfitting(val_metrics['f1_score'], test_metrics['f1_score'])
            })
        
        detailed_df = pd.DataFrame(detailed_data)
        
        # Save to CSV
        report_path = self.results_dir / "detailed_technical_report.csv"
        detailed_df.to_csv(report_path, index=False)
        
        return detailed_df
    
    def _assess_overfitting(self, val_score: float, test_score: float) -> str:
        """Assess overfitting risk based on validation vs test performance."""
        diff = val_score - test_score
        
        if diff > 0.05:
            return "High"
        elif diff > 0.02:
            return "Medium"
        else:
            return "Low"
    
    def create_stakeholder_presentation_plots(self, results: Dict[str, Any]) -> List[str]:
        """Create presentation-ready plots for stakeholders."""
        plot_paths = []
        
        # 1. Model Performance Comparison
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        models = list(results.keys())
        f1_scores = [results[model]['test_metrics']['f1_score'] for model in models]
        roc_aucs = [results[model]['test_metrics']['roc_auc'] for model in models]
        
        # F1-Score comparison
        bars1 = ax1.bar(models, f1_scores, color='skyblue', alpha=0.7)
        ax1.set_title('Model Performance Comparison - F1 Score', fontsize=14, fontweight='bold')
        ax1.set_ylabel('F1 Score', fontsize=12)
        ax1.set_ylim(0, 1)
        ax1.tick_params(axis='x', rotation=45)
        
        # Add value labels on bars
        for bar, score in zip(bars1, f1_scores):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
        
        # ROC-AUC comparison
        bars2 = ax2.bar(models, roc_aucs, color='lightcoral', alpha=0.7)
        ax2.set_title('Model Performance Comparison - ROC AUC', fontsize=14, fontweight='bold')
        ax2.set_ylabel('ROC AUC', fontsize=12)
        ax2.set_ylim(0, 1)
        ax2.tick_params(axis='x', rotation=45)
        
        # Add value labels on bars
        for bar, score in zip(bars2, roc_aucs):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        plot_path1 = self.results_dir / "stakeholder_model_comparison.png"
        plt.savefig(plot_path1, dpi=300, bbox_inches='tight')
        plt.close()
        plot_paths.append(str(plot_path1))
        
        # 2. Best Model Metrics Breakdown
        best_model = max(results.keys(), key=lambda x: results[x]['test_metrics']['f1_score'])
        best_metrics = results[best_model]['test_metrics']
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
        metrics_values = [
            best_metrics['accuracy'],
            best_metrics['precision'],
            best_metrics['recall'],
            best_metrics['f1_score'],
            best_metrics['roc_auc']
        ]
        
        colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#5E4B56']
        bars = ax.barh(metrics_names, metrics_values, color=colors, alpha=0.8)
        
        ax.set_title(f'Best Model ({best_model}) - Performance Metrics', 
                    fontsize=16, fontweight='bold', pad=20)
        ax.set_xlabel('Score', fontsize=12)
        ax.set_xlim(0, 1)
        
        # Add value labels
        for bar, value in zip(bars, metrics_values):
            ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,
                   f'{value:.3f}', va='center', fontweight='bold')
        
        plt.tight_layout()
        plot_path2 = self.results_dir / "best_model_metrics.png"
        plt.savefig(plot_path2, dpi=300, bbox_inches='tight')
        plt.close()
        plot_paths.append(str(plot_path2))
        
        return plot_paths
    
    def generate_feature_importance_report(
        self,
        feature_importance_data: List[Dict[str, Any]],
        model_name: str,
        top_n: int = 10
    ) -> str:
        """Generate feature importance report."""
        if not feature_importance_data:
            return "Feature importance data not available for this model."
        
        # Convert to DataFrame and get top N features
        importance_df = pd.DataFrame(feature_importance_data)
        top_features = importance_df.head(top_n)
        
        # Create visualization
        plt.figure(figsize=(10, 8))
        
        plt.barh(range(len(top_features)), top_features['importance'], 
                color='steelblue', alpha=0.7)
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('Importance Score')
        plt.title(f'Top {top_n} Most Important Features - {model_name}', 
                 fontsize=14, fontweight='bold')
        plt.gca().invert_yaxis()
        
        # Add value labels
        for i, (_, row) in enumerate(top_features.iterrows()):
            plt.text(row['importance'] + max(top_features['importance']) * 0.01, i,
                    f'{row["importance"]:.3f}', va='center', fontweight='bold')
        
        plt.tight_layout()
        plot_path = self.results_dir / f"feature_importance_{model_name}.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # Generate text report
        report = f"""
## FEATURE IMPORTANCE ANALYSIS - {model_name}

The following features have the highest impact on churn prediction:

"""
        for i, (_, row) in enumerate(top_features.iterrows(), 1):
            report += f"{i}. **{row['feature']}**: {row['importance']:.3f}\n"
        
        report += f"""
### Key Insights:
- The top 3 features account for the majority of predictive power
- Focus retention efforts on customers with high values in these key features
- Consider feature engineering to create additional predictive variables

*Feature importance plot saved to: {plot_path}*
"""
        
        return report
    
    def create_comprehensive_stakeholder_package(
        self,
        results: Dict[str, Any],
        best_model_name: str,
        dataset_info: Dict[str, Any]
    ) -> Dict[str, str]:
        """Create a comprehensive package for stakeholders."""
        package_paths = {}
        
        # Executive summary
        summary_path = self.save_executive_summary(results, best_model_name, dataset_info)
        package_paths['executive_summary'] = summary_path
        
        # Detailed technical report
        detailed_df = self.generate_detailed_report(results)
        package_paths['detailed_report'] = str(self.results_dir / "detailed_technical_report.csv")
        
        # Stakeholder plots
        plot_paths = self.create_stakeholder_presentation_plots(results)
        package_paths['plots'] = plot_paths
        
        # Feature importance (if available)
        best_model_results = results[best_model_name]
        if best_model_results.get('feature_importance'):
            importance_report = self.generate_feature_importance_report(
                best_model_results['feature_importance'],
                best_model_name
            )
            importance_path = self.results_dir / f"feature_importance_report_{best_model_name}.md"
            with open(importance_path, 'w') as f:
                f.write(importance_report)
            package_paths['feature_importance'] = str(importance_path)
        
        # Create index file
        index_content = f"""
# Customer Churn Prediction - Stakeholder Package

## Files in this package:

1. **Executive Summary**: {package_paths['executive_summary']}
   - High-level business findings and recommendations

2. **Detailed Technical Report**: {package_paths['detailed_report']}
   - Complete model performance metrics

3. **Presentation Plots**: 
   - {', '.join(plot_paths)}

4. **Feature Importance Analysis**: {package_paths.get('feature_importance', 'Not available')}
   - Key factors driving customer churn

## Best Model: {best_model_name}
- F1-Score: {results[best_model_name]['test_metrics']['f1_score']:.3f}
- ROC-AUC: {results[best_model_name]['test_metrics']['roc_auc']:.3f}

Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        index_path = self.results_dir / "README.md"
        with open(index_path, 'w') as f:
            f.write(index_content)
        package_paths['index'] = str(index_path)
        
        return package_paths
